{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "\n",
    "# ATTS = []\n",
    "# DX = []\n",
    "# X = []\n",
    "\n",
    "# class self_attention(nn.Module):\n",
    "#     \"\"\" Self attention Layer\"\"\"\n",
    "#     def __init__(self,in_dim,q_dim,k_dim,v_dim,activation='relu'):\n",
    "#         super(self_attention,self).__init__()\n",
    "#         self.chanel_in = in_dim\n",
    "#         self.activation = activation\n",
    "        \n",
    "#         # self.query_linear = nn.Linear(in_dim,in_dim)\n",
    "#         # self.key_linear = nn.Linear(in_dim,in_dim)\n",
    "#         # self.value_linear = nn.Linear(in_dim,in_dim)\n",
    "#         # self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "#         self.q_dim = q_dim\n",
    "#         self.k_dim = k_dim\n",
    "#         self.v_dim = v_dim\n",
    "\n",
    "#         self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = q_dim , kernel_size= 1)\n",
    "#         self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = k_dim , kernel_size= 1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = v_dim , kernel_size= 1)\n",
    "#         self.o_proj = nn.Conv2d(in_channels = v_dim , out_channels = in_dim , kernel_size= 1)\n",
    "#         self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "#         self.softmax  = nn.Softmax(dim=-1) \n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         \"\"\"\n",
    "#             inputs :\n",
    "#                 x : input feature maps( B X C X W X H)\n",
    "#             returns :\n",
    "#                 out : self attention value + input feature \n",
    "#                 attention: B X N X N (N is Width*Height)\n",
    "#         \"\"\"\n",
    "#         m_batchsize,C,width ,height = x.size()\n",
    "#         proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "#         proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)        \n",
    "#         energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "#         attention = self.softmax(energy) # BX (N) X (N) \n",
    "#         proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "#         out = torch.bmm(proj_value,attention.permute(0,2,1))\n",
    "#         out = out.view(m_batchsize,self.v_dim,width,height)\n",
    "#         out = self.o_proj(out)\n",
    "\n",
    "#         DX.append(self.gamma*out)\n",
    "#         X.append(x)\n",
    "\n",
    "#         out = self.gamma*out + x\n",
    "\n",
    "#         return out,attention\n",
    "\n",
    "\n",
    "# class net_one_neuron_transformer(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layers_1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=30, kernel_size=(5, 5), stride=(1, 1)),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.BatchNorm2d(30),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout2d(0.3),\n",
    "#             nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(5, 5), stride=(1, 1)),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.BatchNorm2d(30),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout2d(0.3)\n",
    "#          ) #[N,30,9,9] \n",
    "        \n",
    "#         self.attention = self_attention(in_dim=30,q_dim=30,k_dim=30,v_dim=30)\n",
    "\n",
    "#         self.layers_sa_reg = nn.Sequential(\n",
    "#             nn.BatchNorm2d(30),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout2d(0.3)\n",
    "#         )\n",
    "\n",
    "#         self.layers_2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(3, 3), stride=(1, 1)),\n",
    "#             nn.BatchNorm2d(30),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout2d(0.3), #or here\n",
    "#             nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(3, 3), stride=(1, 1)),\n",
    "#             nn.BatchNorm2d(30),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.Linear = nn.Linear(5 * 5 * 30, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layers_1(x)\n",
    "#         x,a = self.attention(x) \n",
    "#         ATTS.append(a)\n",
    "#         x = self.layers_sa_reg(x)\n",
    "#         x = self.layers_2(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.Linear(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class seperate_core_model_transformer(nn.Module):\n",
    "#     def __init__(self,num_neurons):\n",
    "#         super().__init__()\n",
    "#         self.models = nn.ModuleList([net_one_neuron_transformer() for i in range(num_neurons)])\n",
    "#         self.num_neurons = num_neurons\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         outputs = [self.models[i].forward(x) for i in range(self.num_neurons)]\n",
    "#         outputs = torch.stack(outputs, dim=1)\n",
    "#         return outputs.reshape((outputs.shape[0], outputs.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "ATTS = []\n",
    "DX = []\n",
    "X = []\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim,activation='relu'):\n",
    "        super(self_attention,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        # self.query_linear = nn.Linear(in_dim,in_dim)\n",
    "        # self.key_linear = nn.Linear(in_dim,in_dim)\n",
    "        # self.value_linear = nn.Linear(in_dim,in_dim)\n",
    "        # self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)        \n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        DX.append(self.gamma*out)\n",
    "        X.append(x)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out,attention\n",
    "\n",
    "\n",
    "class net_one_neuron_transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=30, kernel_size=(5, 5), stride=(1, 1)),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(5, 5), stride=(1, 1)),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(0.3)\n",
    "         ) #[N,30,9,9] \n",
    "        \n",
    "        self.attention = self_attention(in_dim=30)\n",
    "\n",
    "        self.layers_sa_reg = nn.Sequential(\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "\n",
    "        self.layers_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(0.3), #or here\n",
    "            nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear = nn.Linear(5 * 5 * 30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers_1(x)\n",
    "        x,a = self.attention(x) \n",
    "        ATTS.append(a)\n",
    "        x = self.layers_sa_reg(x)\n",
    "        x = self.layers_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class seperate_core_model_transformer(nn.Module):\n",
    "    def __init__(self,num_neurons):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([net_one_neuron_transformer() for i in range(num_neurons)])\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.models[i].forward(x) for i in range(self.num_neurons)]\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs.reshape((outputs.shape[0], outputs.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from utils import *\n",
    "\n",
    "num_neurons = 10\n",
    "net = seperate_core_model_transformer(num_neurons=num_neurons)\n",
    "net.load_state_dict(torch.load(\"./results/sascnn-2/model.pth\"))\n",
    "\n",
    "vimg = np.load('../all_sites_data_prepared/pics_data/val_img_m1s1.npy')\n",
    "vresp = np.load('../all_sites_data_prepared/New_response_data/valRsp_m1s1.npy')\n",
    "vimg = np.reshape(vimg,(1000,1,50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.from_numpy(np.float32(vimg))\n",
    "img[0:1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    pred_resp = net(img) #[img num, neuron num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test2_pred_resp\",pred_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vresp.shape\n",
    "print(vresp[0:10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(ATTS))\n",
    "\n",
    "# print(X[0][0].flatten())\n",
    "# print(DX[0][0].flatten())\n",
    "# ATTS[0].shape\n",
    "# np.save(\"test2_atts_10\",ATTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as p\n",
    "figure, axis = plt.subplots(2, 5, figsize=(30,10))\n",
    "areas = []\n",
    "for neuron in range(10):\n",
    "    temp =  ATTS[neuron].flatten()\n",
    "    min_max_scaler = p.MinMaxScaler()\n",
    "    temp = min_max_scaler.fit_transform(temp.reshape(-1,1))\n",
    "    temp = sorted(np.array(temp).flatten())\n",
    "    xs = [i/(81**2) for i in range(81**2)]\n",
    "    area = np.trapz(temp,dx=1/(81**2))\n",
    "    areas.append(area)\n",
    "    axis[neuron//5,neuron%5].plot(xs,temp)\n",
    "    axis[neuron//5,neuron%5].set_title(\"1 - AUC =  \" + str(1-area))\n",
    "plt.show()\n",
    "print(str(1 - (sum(areas) / len(areas))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "figure, axis = plt.subplots(5, 6, figsize=(21,14))\n",
    "\n",
    "for neuron in range(5,6):\n",
    "    for channel in range(30):\n",
    "        axis[channel//6,channel%6].scatter(x=X[neuron][0][channel].flatten(),y=DX[neuron][0][channel].flatten(),s=5)\n",
    "        axis[channel//6,channel%6].set_xlabel(\"x\")\n",
    "        axis[channel//6,channel%6].set_ylabel(\"dx\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "figure, axis = plt.subplots(10, 10, figsize=(100,100))\n",
    "\n",
    "for neuron in range(100):\n",
    "    im = axis[neuron//10,neuron%10].imshow(ATTS[5][neuron].detach().cpu().numpy(), cmap='hot', interpolation='nearest')\n",
    "    axis[neuron//10,neuron%10].set_title(\"image number: \" + str(neuron))\n",
    "plt.show()\n",
    "\n",
    "# for neuron in range(10):\n",
    "#     a = ATTS[neuron][0].detach().cpu().numpy()\n",
    "#     plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "#     plt.colorbar()\n",
    "#     plt.title(\"test 8 neuron number: \" + str(neuron))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "figure, axis = plt.subplots(2, 5, figsize=(30,10))\n",
    "\n",
    "for neuron in range(10):\n",
    "    im = axis[neuron//5,neuron%5].imshow(ATTS[neuron][0].detach().cpu().numpy(), cmap='hot', interpolation='nearest')\n",
    "    axis[neuron//5,neuron%5].set_title(\"test 8 neuron number: \" + str(neuron))\n",
    "plt.show()\n",
    "\n",
    "# for neuron in range(10):\n",
    "#     a = ATTS[neuron][0].detach().cpu().numpy()\n",
    "#     plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "#     plt.colorbar()\n",
    "#     plt.title(\"test 8 neuron number: \" + str(neuron))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af67f5ff13b1024a6662e380c335ae332a196a9c31b8678921123ea9d79d5fdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
